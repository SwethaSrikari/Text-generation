{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b953c3ee",
   "metadata": {},
   "source": [
    "# Use pretrained GPT-2 models to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3edd29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/mss/miniconda3/envs/TG/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: importlib-metadata in /Users/mss/miniconda3/envs/TG/lib/python3.7/site-packages (from transformers) (4.11.3)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp37-cp37m-macosx_10_9_x86_64.whl (189 kB)\n",
      "Requirement already satisfied: requests in /Users/mss/miniconda3/envs/TG/lib/python3.7/site-packages (from transformers) (2.28.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mss/miniconda3/envs/TG/lib/python3.7/site-packages (from transformers) (1.21.5)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-macosx_10_11_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 80.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 40.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 18.4 MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.7.9-cp37-cp37m-macosx_10_9_x86_64.whl (289 kB)\n",
      "\u001b[K     |████████████████████████████████| 289 kB 94.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mss/miniconda3/envs/TG/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/mss/miniconda3/envs/TG/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/mss/miniconda3/envs/TG/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/mss/miniconda3/envs/TG/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mss/miniconda3/envs/TG/lib/python3.7/site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mss/miniconda3/envs/TG/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mss/miniconda3/envs/TG/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Installing collected packages: tqdm, pyyaml, filelock, tokenizers, regex, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.7.1 huggingface-hub-0.8.1 pyyaml-6.0 regex-2022.7.9 tokenizers-0.12.1 tqdm-4.64.0 transformers-4.20.1\n"
     ]
    }
   ],
   "source": [
    "# Install transformers\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b447c422",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9edad24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# get transformers\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# To reproduce results\n",
    "SEED = 271\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# number of words (to be predicted) in the output text\n",
    "MAX_LEN = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e3998",
   "metadata": {},
   "source": [
    "## Large GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d73903f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mss/miniconda3/envs/TG/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-07-14 17:18:48.189681: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Downloading: 100%|█████████████████████████| 0.99M/0.99M [00:00<00:00, 8.14MB/s]\n",
      "Downloading: 100%|███████████████████████████| 446k/446k [00:00<00:00, 3.59MB/s]\n",
      "Downloading: 100%|██████████████████████████████| 666/666 [00:00<00:00, 161kB/s]\n",
      "Downloading: 100%|█████████████████████████| 2.88G/2.88G [01:09<00:00, 44.6MB/s]\n",
      "2022-07-14 17:20:00.664134: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfgpt2lm_head_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (TFGPT2MainLaye  multiple                 774030080 \n",
      " r)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 774,030,080\n",
      "Trainable params: 774,030,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# large GPT2 tokenizer and GPT2 model\n",
    "tokenizer_large = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "GPT2_large = TFGPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#view model parameters\n",
    "GPT2_large.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "417ad72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = \"I read a book today. It is so informative and\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc60596",
   "metadata": {},
   "source": [
    "## Generate text using greedy search\n",
    "\n",
    "The greedy search generates words with the highest probabilities and does not look at the diverse possibilities of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f8ef6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I read a book today. It is so informative and so well written. I am so glad I read it. I am so glad I read it. I am so glad I read it. I am so glad I read it. I am so glad I read it. I am so glad I read it. I am so glad I read it. I am so glad I read it. I am so glad I read it. I am so glad I read it. I am so glad I\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer_large.encode(input_sequence, return_tensors='tf')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = GPT2_large.generate(input_ids, max_length = MAX_LEN)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer_large.decode(greedy_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63464447",
   "metadata": {},
   "source": [
    "## Medium GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4435cc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|█████████████████████████| 0.99M/0.99M [00:00<00:00, 6.72MB/s]\n",
      "Downloading: 100%|███████████████████████████| 446k/446k [00:00<00:00, 4.05MB/s]\n",
      "Downloading: 100%|██████████████████████████████| 718/718 [00:00<00:00, 140kB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.32G/1.32G [00:32<00:00, 44.1MB/s]\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfgpt2lm_head_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (TFGPT2MainLaye  multiple                 354823168 \n",
      " r)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354,823,168\n",
      "Trainable params: 354,823,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tokenizer_medium = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "GPT2_medium = TFGPT2LMHeadModel.from_pretrained(\"gpt2-medium\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#view model parameters\n",
    "GPT2_medium.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b426a587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I read a book today. It is so informative and so well written. I am so glad I read it. I am so glad I read it. I am so glad I read it. I am so glad I read it. I am so glad I read it. I am so glad I read it. I am so glad I read it. I am so glad I read it. I am so glad I read it. I am so glad I read it. I am so glad I\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer_medium.encode(input_sequence, return_tensors='tf')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = GPT2_medium.generate(input_ids, max_length = MAX_LEN)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer_medium.decode(greedy_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9f55c3",
   "metadata": {},
   "source": [
    "## Small GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71a2772e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|█████████████████████████| 0.99M/0.99M [00:00<00:00, 5.93MB/s]\n",
      "Downloading: 100%|███████████████████████████| 446k/446k [00:00<00:00, 2.17MB/s]\n",
      "Downloading: 100%|██████████████████████████████| 665/665 [00:00<00:00, 241kB/s]\n",
      "Downloading: 100%|███████████████████████████| 475M/475M [00:10<00:00, 47.1MB/s]\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfgpt2lm_head_model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (TFGPT2MainLaye  multiple                 124439808 \n",
      " r)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124,439,808\n",
      "Trainable params: 124,439,808\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tokenizer_small = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "GPT2_small = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#view model parameters\n",
    "GPT2_small.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ee9dd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I read a book today. It is so informative and so very interesting. I am so glad I read it. I am so glad I read it.\n",
      "\n",
      "I read a book today. It is so informative and so very interesting. I am\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer_small.encode(input_sequence, return_tensors='tf')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = GPT2_small.generate(input_ids, max_length = MAX_LEN)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer_small.decode(greedy_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d42ebdb",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://huggingface.co/blog/how-to-generate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
